{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Spark Object\n",
    "\n",
    "# In Spark 2.0, SparkSession is new entry point to work with RDD, DataFrame and all othere functionalities\n",
    "# Prior 2.0 SparkContext used to be an entry point \n",
    "# \n",
    "# Almost all the APIs available in SparkContext, SQLContext, HiveContext are now available in SparkSession\n",
    "#    SparkContext : Entry point to work with RDD, Accumulators and broadcast variables (< Spark 2.0).\n",
    "#    SQLContext   : Used for initializing the functionalities of Spark SQL (< spark 2.0).\n",
    "#    HiveContext  : Super set of SQLContext (< spark 2.0).\n",
    "# By Default, Spark Shell provides a \"spark\" object which is an instance of SparkSession class\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession \\\n",
    "#        .builder \\\n",
    "#        .master('yarn') \\\n",
    "#        .appName(\"Python Spark SQL basic example\") \\\n",
    "#        .getOrCreate()\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "print(\"Spark Object id created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of partitions for shuffle changed to : \" + str(spark.conf.get('spark.sql.shuffle.partitions')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to create RDD :\n",
    "    ✓ External Data (HDFS, local etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice 1\n",
    "\n",
    "# Create RDD using textFile API\n",
    "rdd = spark.sparkContext.textFile('work/data/PracticeFiles/Customers')\n",
    "rdd.take(5)\n",
    "for i in rdd.take(5): print(i)\n",
    "\n",
    "# Get the Number of Partitions in the RDD\n",
    "Partition_Number = rdd.getNumPartitions()\n",
    "print(Partition_Number)\n",
    "\n",
    "# Get the Number of elements in each partition\n",
    "rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice 2\n",
    "\n",
    "# Create RDD using textFile API and a defined number of partitions\n",
    "rdd = spark.sparkContext.textFile('work/data/PracticeFiles/Customers',10)\n",
    "\n",
    "# Get the Number of Partitions in the RDD\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "# Get the Number of elements in each partition\n",
    "rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to create RDD :\n",
    "    ✓ Local Data\n",
    "    ✓ Python List/Parallelized Collections\n",
    "    ✓ Other RDDs\n",
    "    ✓ Existing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -1\n",
    "\n",
    "# Create a RDD from a Python List\n",
    "\n",
    "lst = [1,2,3,4,5,6,7]\n",
    "rdd = spark.sparkContext.parallelize(lst)\n",
    "for i in rdd.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.sparkContext.parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Practice -2\n",
    "\n",
    "# Create a RDD from local file\n",
    "\n",
    "lst = open('work/data/PracticeFiles/Customers/part-00000').read().splitlines()\n",
    "lst[0:10]\n",
    "rdd = spark.sparkContext.parallelize(lst)\n",
    "for i in rdd.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -3\n",
    "\n",
    "# Create RDD from range function\n",
    "\n",
    "lst1 = range(10)\n",
    "rdd = spark.sparkContext.parallelize(lst1)\n",
    "for i in rdd.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -4\n",
    "\n",
    "# Create RDD from a DataFrame\n",
    "\n",
    "df=spark.createDataFrame(data=(('robert',35),('Mike',45)),schema=('name','age'))\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "rdd1= df.rdd\n",
    "print(type(rdd1))\n",
    "print('-'*30)\n",
    "\n",
    "for i in rdd1.take(2) : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "        Low Level Transformations\n",
    "        (map, flatMap, filter)\n",
    "------------------------------------------\n",
    "\n",
    "# map : map(f, preservesPartitioning=False) \n",
    "        ▪ Perform row level transformations where one record transforms into another record.\n",
    "        ▪ Number of records in input is equal to output.\n",
    "        ▪ Return a new RDD by applying a function to each element of this RDD.\n",
    "        ▪ When we apply a map function to an RDD, a pipelineRDD is formed, \n",
    "          a subclass of RDD. It has all the APIs defined in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files used in this Chapter:\n",
    "\n",
    "'''\n",
    "orders\n",
    "ordItems\n",
    "\n",
    "Please download the orders and ordItems files from Section 2 (Resources).\n",
    "\n",
    "ord = sc.textFile('practice/retail_db/orders')\n",
    "ordItems = sc.textFile('practice/retail_db/order_items')\n",
    "'''\n",
    "\n",
    "# In spark 2.0 and onwards, we can create the a spark object using SparkSession class. \n",
    "# Then using this object we can access the SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load the Files\n",
    "ord = sc.textFile(\"work/data/PracticeFiles/Orders\")\n",
    "ordItems  = sc.textFile(\"work/data/PracticeFiles/Order_items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -1\n",
    "#   PS: Project all the Order_ids.\n",
    "\n",
    "ordMap = ord.map(lambda x : x.split(','))\n",
    "for i in ordMap.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordItems = ordItems.map(lambda x : x.split(','))\n",
    "for i in ordItems.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordMap = ord.map(lambda x : x.split(',')[0])\n",
    "for i in ordMap.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -2\n",
    "#   PS: Project all the Orders and their status.\n",
    "\n",
    "ordMap = ord.map(lambda x : (x.split(',')[0],x.split(',')[3])).take(5)\n",
    "for i in ordMap : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practice -3\n",
    "#   PS: Combine Order id and status with '#'\n",
    "\n",
    "ordMap = ord.map(lambda x : x.split(',')[0] + '#' + x.split(',')[3]).take(5)\n",
    "for i in ordMap : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practice -4\n",
    "#   PS: Convert the Order date into YYYY/MM/DD Format.\n",
    "\n",
    "ordMap = ord.map(lambda x : x.split(',')[1].split(' ')[0].replace('-','/')).first()\n",
    "print(ordMap)\n",
    "# for i in ordMap : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordMap = ord.map(lambda x : x.split(',')[1].split(' ')[1].replace(':','/')).first()\n",
    "print(ordMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practice -5\n",
    "#   PS: Create key-value pairs with key as Order id and values as whole records.\n",
    "\n",
    "ordMap = ord.map(lambda x : (x.split(',')[0],x)).take(5)\n",
    "for i in ordMap : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practice -6 \n",
    "#   PS: Project all the Order_item_ids and their subtotal.\n",
    "\n",
    "#ordItemsMap = ordItems.map(lambda x : (x.split(',')[0],x.split(',')[4]))\n",
    "#ordItemsMap.take(5)\n",
    "\n",
    "ordItemsMap = ordItems.map(lambda x : (x.split(',')[0],x.split(',')[4])).take(5)\n",
    "for i in ordItemsMap : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practice -7\n",
    "#   PS: Applied user defined function to convert status into lowercase.\n",
    "\n",
    "def lowerCase(str):\n",
    "    return str.lower()   \n",
    "\n",
    "ord.map(lambda x : lowerCase(x.split(',')[3])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap : flatMap(f, preservesPartitioning=False)\n",
    "    ▪ Return a new RDD by first applying a function to all elements of this RDD, \n",
    "      and then flattening the results.\n",
    "    ▪ Similar to map, but each input item can be mapped to 0 or more output items \n",
    "      (so func should return a Seq rather than a single item). \n",
    "      Number of records in input is less than or equal to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 2)\n",
      "('CLOSED', 7556)\n",
      "('256', 11)\n",
      "('12111', 7)\n",
      "('4', 7)\n",
      "('11318', 7)\n",
      "('7130', 8)\n",
      "('8', 9)\n",
      "('2911', 7)\n",
      "('9', 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "##### Practice -1\n",
    "#   PS : Word count in orders file.\n",
    "\n",
    "wordCount = ord.flatMap(lambda x : x.split(',')).map(lambda w : (w, 1)).reduceByKey(lambda x, y : x + y)\n",
    "for i in wordCount.take(10) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2013-07-25 00:00:00.0', '11599', 'CLOSED', '2', '2013-07-25 00:00:00.0']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = ord.flatMap(lambda x : x.split(','))#.map(lambda w : (w, 1)).reduceByKey(lambda x, y : x + y)\n",
    "cf.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2013-07-25 00:00:00.0', '11599', 'CLOSED'],\n",
       " ['2', '2013-07-25 00:00:00.0', '256', 'PENDING_PAYMENT'],\n",
       " ['3', '2013-07-25 00:00:00.0', '12111', 'COMPLETE'],\n",
       " ['4', '2013-07-25 00:00:00.0', '8827', 'CLOSED'],\n",
       " ['5', '2013-07-25 00:00:00.0', '11318', 'COMPLETE'],\n",
       " ['6', '2013-07-25 00:00:00.0', '7130', 'COMPLETE']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = ord.map(lambda x : x.split(','))\n",
    "cf.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25882,2014-01-01 00:00:00.0,4598,COMPLETE',\n",
       " '25888,2014-01-01 00:00:00.0,6735,COMPLETE',\n",
       " '25889,2014-01-01 00:00:00.0,10045,COMPLETE',\n",
       " '25891,2014-01-01 00:00:00.0,3037,CLOSED',\n",
       " '25895,2014-01-01 00:00:00.0,1044,COMPLETE']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Practice -1\n",
    "#   PS: Print all the orders which are closed or Complete and ordered in the year 2013.\n",
    "#   Return a new dataset formed by selecting those elements of the source on which func returns true.\n",
    "\n",
    "filteredOrd = ord.filter(lambda x : (x.split(',')[3] in (\"CLOSED\",\"COMPLETE\"))  \\\n",
    "                                and (x.split(',')[1].split('-')[0] == '2014'))\n",
    "filteredOrd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 3), ('a', 5)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Practice -1\n",
    "rdd = sc.parallelize(((\"a\", (1,2,3)), (\"b\", (3,4,5)),(\"a\", (1,2,3,4,5))))\n",
    "def f(x): return len(x)\n",
    "rdd.mapValues(f).collect()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
