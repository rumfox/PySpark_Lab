{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Spark Object\n",
    "\n",
    "# In Spark 2.0, SparkSession is new entry point to work with RDD, DataFrame and all othere functionalities\n",
    "# Prior 2.0 SparkContext used to be an entry point \n",
    "# \n",
    "# Almost all the APIs available in SparkContext, SQLContext, HiveContext are now available in SparkSession\n",
    "#    SparkContext : Entry point to work with RDD, Accumulators and broadcast variables (< Spark 2.0).\n",
    "#    SQLContext   : Used for initializing the functionalities of Spark SQL (< spark 2.0).\n",
    "#    HiveContext  : Super set of SQLContext (< spark 2.0).\n",
    "# By Default, Spark Shell provides a \"spark\" object which is an instance of SparkSession class\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession \\\n",
    "#        .builder \\\n",
    "#        .master('yarn') \\\n",
    "#        .appName(\"Python Spark SQL basic example\") \\\n",
    "#        .getOrCreate()\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "print(\"Spark Object id created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of partitions for shuffle changed to : \" + str(spark.conf.get('spark.sql.shuffle.partitions')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to create RDD :\n",
    "    ✓ External Data (HDFS, local etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice 1\n",
    "\n",
    "# Create RDD using textFile API\n",
    "rdd = spark.sparkContext.textFile('work/data/PracticeFiles/Customers')\n",
    "rdd.take(5)\n",
    "for i in rdd.take(5): print(i)\n",
    "\n",
    "# Get the Number of Partitions in the RDD\n",
    "Partition_Number = rdd.getNumPartitions()\n",
    "print(Partition_Number)\n",
    "\n",
    "# Get the Number of elements in each partition\n",
    "rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice 2\n",
    "\n",
    "# Create RDD using textFile API and a defined number of partitions\n",
    "rdd = spark.sparkContext.textFile('work/data/PracticeFiles/Customers',10)\n",
    "\n",
    "# Get the Number of Partitions in the RDD\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "# Get the Number of elements in each partition\n",
    "rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to create RDD :\n",
    "    ✓ Local Data\n",
    "    ✓ Python List/Parallelized Collections\n",
    "    ✓ Other RDDs\n",
    "    ✓ Existing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -1\n",
    "\n",
    "# Create a RDD from a Python List\n",
    "\n",
    "lst = [1,2,3,4,5,6,7]\n",
    "rdd = spark.sparkContext.parallelize(lst)\n",
    "for i in rdd.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.sparkContext.parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Practice -2\n",
    "\n",
    "# Create a RDD from local file\n",
    "\n",
    "lst = open('work/data/PracticeFiles/Customers/part-00000').read().splitlines()\n",
    "lst[0:10]\n",
    "rdd = spark.sparkContext.parallelize(lst)\n",
    "for i in rdd.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -3\n",
    "\n",
    "# Create RDD from range function\n",
    "\n",
    "lst1 = range(10)\n",
    "rdd = spark.sparkContext.parallelize(lst1)\n",
    "for i in rdd.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -4\n",
    "\n",
    "# Create RDD from a DataFrame\n",
    "\n",
    "df=spark.createDataFrame(data=(('robert',35),('Mike',45)),schema=('name','age'))\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "rdd1= df.rdd\n",
    "print(type(rdd1))\n",
    "print('-'*30)\n",
    "\n",
    "for i in rdd1.take(2) : print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "        Low Level Transformations\n",
    "        (map, flatMap, filter)\n",
    "------------------------------------------\n",
    "\n",
    "map : map(f, preservesPartitioning=False) \n",
    "▪ Perform row level transformations where one record transforms into another record.\n",
    "▪ Number of records in input is equal to output.\n",
    "▪ Return a new RDD by applying a function to each element of this RDD.\n",
    "▪ When we apply a map function to an RDD, a pipelineRDD is formed, a subclass of RDD. It has all the APIs defined in \n",
    "the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files used in this Chapter:\n",
    "\n",
    "'''\n",
    "orders\n",
    "ordItems\n",
    "\n",
    "Please download the orders and ordItems files from Section 2 (Resources).\n",
    "\n",
    "ord = sc.textFile('practice/retail_db/orders')\n",
    "ordItems = sc.textFile('practice/retail_db/order_items')\n",
    "'''\n",
    "# Load the Files\n",
    "ord = spark.textFile(\"work/data/PracticeFiles/PracticeFiles/Orders\")\n",
    "ordItems  = spark.textFile(\"work/data/PracticeFiles/PracticeFiles/Order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--Map Function\n",
    "##### Practice -1\n",
    "PS: Project all the Order_ids.\n",
    "ordMap = ord.map(lambda x : x.split(',')[0])\n",
    "for i in ordMap.take(5) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -2\n",
    "PS: Project all the Orders and their status.\n",
    "ord.map(lambda x : (x.split(',')[0],x.split(',')[3])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practice -3\n",
    "PS: Combine Order id and status with '#'\n",
    "ord.map(lambda x : x.split(',')[0] + '#' + x.split(',')[3]).take(5)\n",
    "\n",
    "### Practice -4\n",
    "PS: Convert the Order date into YYYY/MM/DD Format.\n",
    "ord.map(lambda x : x.split(',')[1].split(' ')[0].replace('-','/')).first()\n",
    "\n",
    "### Practice -5\n",
    "PS: Create key-value pairs with key as Order id and values as whole records.\n",
    "ordMap = ord.map(lambda x : (x.split(',')[0],x))\n",
    "ordMap.take(5)\n",
    "\n",
    "### Practice -6 \n",
    "PS: Project all the Order_item_ids and their subtotal.\n",
    "ordItemsMap = ordItems.map(lambda x : (x.split(',')[0],x.split(',')[4]))\n",
    "ordItemsMap.take(5)\n",
    "\n",
    "### Practice -7\n",
    "PS: Applied user defined function to convert status into lowercase.\n",
    "def lowerCase(str):\n",
    "    return str.lower()   \n",
    "ord.map(lambda x : lowerCase(x.split(',')[3])).first()\n",
    "\n",
    "###### END ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--Files used in this Chapter:\n",
    "orders\n",
    "\n",
    "Please download the orders  files from Section 2 (Resources).\n",
    "\n",
    "--Load the Files\n",
    "ord = sc.textFile('practice/retail_db/orders')\n",
    "\n",
    "--flatMap Function\n",
    "##### Practice -1\n",
    "### PS : Word count in orders file.\n",
    "wordCount = ord.flatMap(lambda x : x.split(',')).map(lambda w : (w,1)).reduceByKey(lambda x,y : x+y)\n",
    "for i in wordCount.take(20) : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--Files used in this Chapter:\n",
    "orders\n",
    "\n",
    "Please download the orders  files from Section 2 (Resources).\n",
    "\n",
    "--Load the Files\n",
    "ord = sc.textFile('practice/retail_db/orders')\n",
    "\n",
    "--filter Function\n",
    "##### Practice -1\n",
    "PS: Print all the orders which are closed or Complete and ordered in the year 2013.\n",
    "filteredOrd = ord.filter(lambda x : (x.split(',')[3] in (\"CLOSED\",\"COMPLETE\"))  and (x.split(',')[1].split('-')[0] == '2014'))\n",
    "filteredOrd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Practice -1\n",
    "rdd = sc.parallelize(((\"a\", (1,2,3)), (\"b\", (3,4,5)),(\"a\", (1,2,3,4,5))))\n",
    "def f(x): return len(x)\n",
    "rdd.mapValues(f).collect()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
